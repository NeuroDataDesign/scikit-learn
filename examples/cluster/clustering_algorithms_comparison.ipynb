{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of different clustering algorithms on toy datasets on adding high dimensional Gaussian noise\n",
    "\n",
    "This experiment analyzes how different clustering algorithms perform on different datasets that are originally two dimensional when high dimensional Gaussian noise is added to them. The experiment also shows how the performance of the clutstering algorithms in the above simulations is affected when the variance of the Gaussian noise added is varied. This experiment is influenced by Scikitlearn's demo on clustering, 'Comparing different clustering algorithms on toy datasets'and is carried out with the aim of analyzing the behavior of different clustering algoithms when dealing with high dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating toy datasets and defining the parameters for the clustering algorithms\n",
    "\n",
    "The clustering algorithms whose performance is being evaluated are k-means clustering, agglomerative clustering, DBSCAN, GMM, spectral clustering, and affinity propagation. \n",
    "\n",
    "The 2D toy datasest are generated using make_blobs,make_classification, make_moons, and make_circles from sklearn.datasets. The generated datasets all of which have 500 samples with 2 features that belong to 2 classes are normalized before noise addition so as to make the variance 1, and centralize the data points around zero which enables easier parameter selection while clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''defining datasets'''\n",
    "noise_dimensions=np.arange(0,110,10)\n",
    "noise_levels=[0.1,1,10]\n",
    "noise_range=len(noise_dimensions)\n",
    "trial_num=100\n",
    "parameters={'samples':500,'features':2,'clusters':2,'eps': .3,'damping':0.9,'preference':-200}\n",
    "\n",
    "initial_blobs = datasets.make_blobs(n_samples=parameters['samples'], \n",
    "                            centers=parameters['clusters'],\n",
    "                            cluster_std=1,\n",
    "                            random_state=8)\n",
    "\n",
    "initial_moons = datasets.make_moons(n_samples=parameters['samples'], \n",
    "                            noise=0.06)\n",
    "initial_circles = datasets.make_circles(n_samples=parameters['samples'], \n",
    "                                factor=0.5,\n",
    "                                noise=0.06)\n",
    "initial_classification = datasets.make_classification(n_samples=parameters['samples'], \n",
    "                                  n_features=parameters['features'],\n",
    "                                  n_informative=parameters['features'],\n",
    "                                  n_redundant=0,\n",
    "                                  n_repeated=0,\n",
    "                                  n_classes=parameters['clusters'],\n",
    "                                  n_clusters_per_class=1,\n",
    "                                  flip_y=0.1,\n",
    "                                  random_state=8,\n",
    "                                  shuffle=True)\n",
    "\n",
    "'''Normalizing all the datasets for easier parameter selection'''\n",
    "def standardize(input_data): # input should be the dataset\n",
    "    temp = preprocessing.StandardScaler().fit_transform(input_data[0])\n",
    "    standardized_input = []\n",
    "    standardized_input.append(temp)\n",
    "    standardized_input.append(input_data[1])\n",
    "    return standardized_input\n",
    "\n",
    "blobs = standardize(initial_blobs)\n",
    "classification = standardize(initial_classification)\n",
    "circles = standardize(initial_circles)\n",
    "moons = standardize(initial_moons)\n",
    "\n",
    "datasets = [blobs,classification,circles,moons]\n",
    "\n",
    "\n",
    "names=['make_blobs','make_classification','make_circles','make_moons']\n",
    "\n",
    "clustering_algorithms = [cluster.KMeans(n_clusters=parameters['clusters']),\n",
    "                         cluster.AgglomerativeClustering(n_clusters=parameters['clusters'],linkage='ward'),\n",
    "                         cluster.DBSCAN(eps=parameters['eps']),\n",
    "                         mixture.GaussianMixture(n_components=parameters['clusters'],covariance_type='full'),\n",
    "                         cluster.SpectralClustering(n_clusters=parameters['clusters'],eigen_solver='arpack',affinity='nearest_neighbors'),\n",
    "                         cluster.AffinityPropagation(damping=parameters['damping'],preference=parameters['preference'])]\n",
    "clustering_names=['K-means Clustering','Agglomerative Clustering','DBSCAN','GMM','Spectral Clustering','Affinity Propagation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding noise and computing the accuracy\n",
    "In each simulation different clustering algorithms are performed after Gaussian noise of dimension n is added to the generated datasets where n=0,10,...,100. The performance of the clustering algorithms is measured using adjusted rand index (ARI) as the metric. \n",
    "\n",
    "The above experiment is carried out by changing the variance of the added noise to analyze also the effect of increasing variance in the above simulations. A noise with standard deviation of 0.1 is regarded as having low variance, 1 as having moderate variance, and 10 as having high variance in this context. The Gaussian noise added is always centered around zero. There are 100 trials for this experiment, and the mean and variance resulting from these 100 trails are used in plotting the results so as to minimize random effects in the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Adding high dimensional noise'''\n",
    "def addNoise(x,noise_dim,noise_std,samples=parameters['samples'],noise_mean=0):\n",
    "    temp_noise=np.random.normal(noise_mean,noise_std,[samples,noise_dim])\n",
    "    data=np.concatenate((x,temp_noise),axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(algorithm,data_X,data_y):\n",
    "    cluster=algorithm.fit(data_X)\n",
    "    label=cluster.labels_\n",
    "    ARI=adjusted_rand_score(label,data_y)\n",
    "    return ARI\n",
    "\n",
    "def accuracyGaussian(algorithm,data_X,data_y):\n",
    "    cluster=algorithm.fit(data_X)\n",
    "    label=cluster.predict(data_X)\n",
    "    ARI=adjusted_rand_score(label,data_y)\n",
    "    return ARI\n",
    "\n",
    "def toPlot(noise_std):\n",
    "    ARI=np.zeros((len(datasets),len(clustering_algorithms),trial_num,noise_range))\n",
    "    all_trial_ARI=np.zeros((len(datasets),len(clustering_algorithms),noise_range*trial_num)) # to create dataframe\n",
    "    the_noise_dim=np.zeros((len(datasets),len(clustering_algorithms),noise_range*trial_num))\n",
    "    all_ARI=[[],[],[],[]]\n",
    "    event=[[],[],[],[]]\n",
    "    all_noise=[[],[],[],[]]\n",
    "    for index,dataset in enumerate(datasets):\n",
    "        for ind,algorithm in enumerate(clustering_algorithms):\n",
    "            for trial in range(trial_num):\n",
    "                for noise_index,noise in enumerate(noise_dimensions):\n",
    "                    new_X=addNoise(dataset[0],noise,noise_std)\n",
    "                    if ind==3:\n",
    "                        ARI[index][ind][trial][noise_index]=accuracyGaussian(algorithm,new_X,dataset[1])\n",
    "                    else:\n",
    "                        ARI[index][ind][trial][noise_index]=accuracy(algorithm,new_X,dataset[1])\n",
    "                    all_trial_ARI[index][ind][noise_index+trial*noise_range]=ARI[index][ind][trial][noise_index]\n",
    "                    the_noise_dim[index][ind][(noise_index)+trial*noise_range]=noise\n",
    "                    #to create a combined plot\n",
    "                    all_ARI[index].append(ARI[index][ind][trial][noise_index])\n",
    "                    event[index].append(clustering_names[ind])\n",
    "                    all_noise[index].append(noise)\n",
    "                    \n",
    "    return all_noise,event,all_ARI\n",
    "\n",
    "all_noise_1,event_1,all_ARI_1=toPlot(0.1) # arrays corresponding to noise_std=0.1\n",
    "all_noise_2,event_2,all_ARI_2=toPlot(1) # arrays corresponding to noise_std=1\n",
    "all_noise_3,event_3,all_ARI_3=toPlot(10) # arrays corresponding to noise_std=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the number of noise dimensions against ARI enables a better visualization of change in the performance of each clustering algorithm with the number of noise dimensions for different datasets. The three plots that follow give a better intuition on how the whole scenario changes when dealing with noise of three different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "'''Plotting the data'''\n",
    "f, axes = plt.subplots(len(noise_levels)+1, len(datasets), figsize=(25, 20), sharex=True,sharey=True)\n",
    "for index,(dataset,name) in enumerate(zip(datasets,names)):\n",
    "    plt.subplot(len(noise_levels)+1,len(datasets),index+1)\n",
    "    ax=sns.scatterplot(dataset[0][:,0],dataset[0][:,1],hue=dataset[1],edgecolor='black')\n",
    "    plt.title(name)\n",
    "    plt.tight_layout()    \n",
    "    plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    if(index+1!=len(datasets)):\n",
    "        ax.legend_.remove()\n",
    "    plt.ylim(-2.5,2.5)\n",
    "    plt.xlim(-3.3,3.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "all_ARI=[all_ARI_1,all_ARI_2,all_ARI_3]\n",
    "all_noise=[all_noise_1,all_noise_2,all_noise_3]\n",
    "event=event_1 # event_1, event_2, and event_3 are all the same arrays\n",
    "def plotAll(z,dimension,rand_index,dataset,noise_level,event=event[0],x=len(noise_levels)+1,y=len(datasets)):\n",
    "    plt.subplot(x,y,z)\n",
    "    data={'Number of Noise Dimensions':dimension,'ARI':rand_index,'Event':event}\n",
    "    ax=sns.lineplot(x='Number of Noise Dimensions',y='ARI',hue='Event',data=data)\n",
    "    if (z%len(datasets)==1):\n",
    "        plt.ylabel(' noise_std='+ str(noise_level))\n",
    "    else:\n",
    "        plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "    if(z<=(len(noise_levels))*len(datasets)):\n",
    "        plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "    if(z<=len(datasets)):\n",
    "        plt.title(dataset)    \n",
    "    plt.tight_layout()\n",
    "    if(z!=len(datasets)*2):\n",
    "        ax.legend_.remove()\n",
    "    if(z==len(datasets)*2):\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.ylim(-0.1,1.1)\n",
    "    return data\n",
    "\n",
    "f.text(0.425, -0.012, 'Number of Noise Dimensions', ha='center',color='b',fontsize=22)\n",
    "f.text(-0.022, 0.385, 'ARI', va='center', rotation='vertical',color='b',fontsize=22)\n",
    "for index,noise_level in enumerate(noise_levels):\n",
    "    for ind,(dataset,name) in enumerate(zip(datasets,names)):\n",
    "        plotAll(index*len(datasets)+5+ind,all_noise[index][ind],all_ARI[index][ind],name,noise_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result interpretation\n",
    "\n",
    "When the noise standard deviation is low (noise_std=0.1), spectral clustering seems to give the best performance. Spectral clustering has a near perfect ARI when applied to make_blobs and make_moons datasets and this doesn't seem to be affected on increasing the noise dimension from 0 to 100. Also, it shows better performance compared to other clustering algoithms in the case of make_classification. To add, this is the only algorithm that does a good job in clustering in the case of make_circles other than DBSCAN. Here while spectral clustering has a near-perfect ARI on the addition of upto 50 noise dimensions, the accuracy of DBSCAN drops to zero soon after noise of any dimension is added. The performances of agglomerative clustering, affinity propagation, and k-means clustering algorithms don't seem to change on increasing the noise dimensions but at the same time give out the worst performance with ARI nearly equal to zero in the case of make_circles. Although GMM is similar to these thre algorithms in performance, it is affected more by the addition of increasing noise dimensions. \n",
    "\n",
    "\n",
    "DBSCAN performs consistently poorly on applying to make_classification. In other datasets although it has a near perfect ARI when no noise dimension is added, the performance plummets down and reaches close to zero as soon as noise of any dimension is added. In summary, spectral clustering does a better job compared to other algorithms under consideration in terms of performance and susceptibility to increasing addition of noise dimensions.\n",
    "\n",
    "When the noise standard deviation is moderate (noise_std=1), spectral clustering again performs better compared to others and is comparatively less susceptible to increasing noise dimesions. All algorithms seem to fail in clustering when noise of any dimension with high variance (noise_std=10) is added.\n",
    "\n",
    "To summarize, spectral clustering puts forth a better performance when noise with moderate or low variance is added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
