{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_suite_CC18 = openml.study.get_suite('OpenML-CC18')  # obtain the benchmark suite\n",
    "benchmark_suite_100 = openml.study.get_suite('OpenML100')  # obtain the benchmark suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark RandomForestClassifier and ExtraTreesClassifier with _default parameters_\n",
    "\n",
    "#### The following sections use the OpenML CC-18 and 100 suites and classifies each dataset using the sklearn's RandomForest classifier and ExtraTrees classifier with default parameters. The task IDs, accuracies, and runtimes of each dataset are compiled into .txt files for later analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for building classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(classifier_name):\n",
    "    if classifier_name == \"RF\":\n",
    "        clf = sklearn.pipeline.make_pipeline(sklearn.preprocessing.Imputer(),\n",
    "                                     sklearn.ensemble.RandomForestClassifier())\n",
    "        return clf\n",
    "    elif classifier_name == \"ET\":\n",
    "        clf = sklearn.pipeline.make_pipeline(sklearn.preprocessing.Imputer(),\n",
    "                                     sklearn.ensemble.ExtraTreesClassifier())\n",
    "        return clf\n",
    "    else:\n",
    "        return None       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for collecting dataset accuracies and runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies_runtimes(dataset_suite, accuracy_txt, api_key, clf):\n",
    "    f = open(accuracy_txt, 'w').close()\n",
    "    for task_id in dataset_suite.tasks:  # iterate over all tasks\n",
    "        try:\n",
    "            f = open(accuracy_txt,\"a\")\n",
    "            startTime = datetime.now()\n",
    "            task = openml.tasks.get_task(task_id) # download the OpenML task\n",
    "            openml.config.apikey = api_key  # set the OpenML Api Key\n",
    "            run = openml.runs.run_model_on_task(clf, task) # run classifier on splits (requires API key)\n",
    "            score = run.get_metric_fn(sklearn.metrics.accuracy_score) # print accuracy score\n",
    "            f.write('%i,%s,%0.4f,%s,\\n' % (task_id,task.get_dataset().name,score.mean(),str(datetime.now() - startTime)))\n",
    "            f.close()\n",
    "        except:\n",
    "            print('Error in' + task.get_dataset().name + str(task_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'c9ea8896542dd998ea42685f14e2bc14'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the scikit-learn RandomForest classifier\n",
    "clf = build_classifier(\"RF\")\n",
    "\n",
    "# RF using OpenML CC-18\n",
    "get_accuracies_runtimes(benchmark_suite_CC18, \"RF_accuracies_runtimes_CC-18.txt\", api_key, clf)\n",
    "\n",
    "# RF using OpenML 100\n",
    "get_accuracies_runtimes(benchmark_suite_100, \"RF_accuracies_runtimes_100.txt\", api_key, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"OpenMLBenchmarkSuite\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenMLServerException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8b69ba76adbe>\u001b[0m in \u001b[0;36mget_accuracies_runtimes\u001b[0;34m(dataset_suite, accuracy_txt, api_key, clf)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# download the OpenML task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapikey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_key\u001b[0m  \u001b[0;31m# set the OpenML Api Key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/tasks/functions.py\u001b[0m in \u001b[0;36mget_task\u001b[0;34m(task_id, download_data)\u001b[0m\n\u001b[1;32m    377\u001b[0m         )\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/tasks/functions.py\u001b[0m in \u001b[0;36mget_task\u001b[0;34m(task_id, download_data)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_task_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;31m# List of class labels availaible in dataset description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(dataset_id, download_data, version, error_if_multiple)\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(dataset_id, download_data, version, error_if_multiple)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dataset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dataset_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mqualities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dataset_qualities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdid_cache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m_get_dataset_features\u001b[0;34m(did_cache_dir, dataset_id)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0murl_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/features/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mfeatures_xml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_extension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'get'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/_api_calls.py\u001b[0m in \u001b[0;36m_perform_api_call\u001b[0;34m(call, request_method, data, file_elements)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read_url_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_elements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tealeaf/lib/python3.7/site-packages/openml/_api_calls.py\u001b[0m in \u001b[0;36m_read_url\u001b[0;34m(url, request_method, data)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_parse_server_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Content-Encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOpenMLServerException\u001b[0m: https://www.openml.org/api/v1/xml/data/features/1176 returned code 274: No features found. Additionally, dataset processed with error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4f667bdb6331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# ET using OpenML 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mget_accuracies_runtimes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_suite_100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ET_accuracies_runtimes_100.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8b69ba76adbe>\u001b[0m in \u001b[0;36mget_accuracies_runtimes\u001b[0;34m(dataset_suite, accuracy_txt, api_key, clf)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error in'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset_suite\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"OpenMLBenchmarkSuite\") to str"
     ]
    }
   ],
   "source": [
    "# Build the scikit-learn ExtraTrees classifier\n",
    "clf = build_classifier(\"ET\")\n",
    "\n",
    "# ET using OpenML CC-18\n",
    "get_accuracies_runtimes(benchmark_suite_CC18, \"ET_accuracies_runtimes_CC-18.txt\", api_key, clf)\n",
    "\n",
    "# ET using OpenML 100\n",
    "get_accuracies_runtimes(benchmark_suite_100, \"ET_accuracies_runtimes_100.txt\", api_key, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare accuracies and runtimes between RF and ET classifiers\n",
    "\n",
    "#### The following sections compare the compiled accuracy and runtime results of both classifiers on the OpenML CC-18 and 100 suites. The accuracies and runtimes of each dataset are plotted against the dataset's number of features and number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to collect number of samples and features for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampleNum_featureNum(dataset_suite):\n",
    "    dimensions = [] # for all datasets: number of samples (column 0), number of features (column 1) \n",
    "\n",
    "    for task_id in dataset_suite.tasks:  # iterate over all tasks\n",
    "        try:\n",
    "            task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "            features, targets = task.get_X_and_y()  # load in dataset properties\n",
    "            dimensions.append(np.shape(features)) # compile all feature and sample numbers\n",
    "        except:\n",
    "            print('Error in dataset ' + str(task_id)) # print task id of erring datasets\n",
    "            \n",
    "    return dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to read in each dataset's accuracies and runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_accuracies_runtimes(accuracy_txt):\n",
    "    acc = open(accuracy_txt,'r')\n",
    "\n",
    "    taskID = [] # read in taskID for each dataset\n",
    "    name = [] # read in name for each dataset\n",
    "    accuracy = [] # read in RF accuracy for each dataset\n",
    "    runtime = [] # read in RF runtime for each dataset\n",
    "\n",
    "    for line in acc:\n",
    "        fields = line.split(',')\n",
    "        taskID.append(fields[0])\n",
    "        name.append(fields[1])\n",
    "        accuracy.append(float(fields[2]))\n",
    "        runtime.append(fields[3])\n",
    "    \n",
    "    return taskID, name, accuracy, runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dataset properties, accuracies, and runtimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain the properties of datasets in Open ML CC-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimen_CC18 = get_sampleNum_featureNum(benchmark_suite_CC18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Obtain the properties of datasets in Open ML 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimen_100 = get_sampleNum_featureNum(benchmark_suite_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in RF accuracies of OpenML CC-18 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_taskID_CC18, RF_name_CC18, RF_accuracy_CC18, RF_time_CC18 = read_accuracies_runtimes(\"RF_accuracies_runtimes_CC-18.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in RF accuracies of OpenML 100 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_taskID_100, RF_name_100, RF_accuracy_100, RF_time_100 = read_accuracies_runtimes(\"RF_accuracies_runtimes_100.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in ET accuracies of OpenML CC-18 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_taskID_CC18, ET_name_CC18, ET_accuracy_CC18, ET_time_CC18 = read_accuracies_runtimes(\"ET_accuracies_runtimes_CC-18.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read in ET accuracies of OpenML 100 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_taskID_100, ET_name_100, ET_accuracy_100, ET_time_100 = read_accuracies_runtimes(\"ET_accuracies_runtimes_100.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find RF vs. ET accuracy differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect accuracy differences (ET-RF) for each dataset in OpenML CC-18\n",
    "acc_diffs_CC18 = []\n",
    "\n",
    "for i in range(len(ET_accuracy_CC18)):\n",
    "    acc_diffs_CC18.append(ET_accuracy_CC18[i]-RF_accuracy_CC18[i])\n",
    "\n",
    "# collect accuracy differences (ET-RF) for each dataset in OpenML 100\n",
    "acc_diffs_100 = []\n",
    "\n",
    "for i in range(len(ET_accuracy_100)):\n",
    "    acc_diffs_100.append(ET_accuracy_100[i]-RF_accuracy_100[i])\n",
    "\n",
    "# plot histogram of difference values\n",
    "sns.distplot(acc_diffs_CC18, color = \"skyblue\", label = \"CC-18\", rug = True)\n",
    "sns.distplot(acc_diffs_100, color = \"red\", label = \"100\", rug = True)\n",
    "plt.axvline(x = 0, color = 'k', linestyle = \"--\")\n",
    "plt.text(0.01, 40, \"ET better\", fontsize=12)\n",
    "plt.text(-0.03, 40, \"RF better\", fontsize=12)\n",
    "plt.xlabel('Accuracy Difference')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot RF vs. ET accuracy differences (scatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenML CC-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up datapoints with the accuracy differences vs. number of features and number of samples, separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_CC18 = []\n",
    "n_samples_CC18 = []\n",
    "\n",
    "for i in range(len(ET_time_CC18)):\n",
    "    n_features_CC18.append(dimen_CC18[i][1])\n",
    "    n_samples_CC18.append(dimen_CC18[i][0])\n",
    "\n",
    "n_features_CC18 = np.vstack((n_features_CC18, acc_diffs_CC18))\n",
    "n_samples_CC18 = np.vstack((n_samples_CC18, acc_diffs_CC18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function to plot each dataset's accuracy difference against either number of features or number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_diff(dataset_suite, f_s_nums, f_or_s):\n",
    "    df = pd.DataFrame(f_s_nums.transpose(), columns=[\"Number of \" + f_or_s, \"Accuracy differences (ET - RF): \" + dataset_suite])\n",
    "\n",
    "    g = sns.JointGrid('Number of ' + f_or_s, 'Accuracy differences (ET - RF): ' + dataset_suite, df)\n",
    "    g.plot_joint(plt.scatter, color='black', edgecolor='black')\n",
    "    g.plot_marginals(sns.distplot, hist=True, kde=True, color='blue')\n",
    "    ax = g.ax_joint\n",
    "    ax.set_xscale('log')\n",
    "    g.ax_marg_x.set_xscale('log')\n",
    "    ax.axhline(y=0, color = 'r', linestyle = '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's accuracy difference against its number of features and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_diff(\"CC18\", n_features_CC18, \"features\")\n",
    "\n",
    "plot_acc_diff(\"CC18\", n_samples_CC18, \"samples\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's accuracy difference against its number of samples and number of features, with the marker size denoting the relative difference magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_CC18 = 0\n",
    "plt.figure()\n",
    "for i in range(len(acc_diffs_CC18)):\n",
    "    if acc_diffs_CC18[i] > 0:\n",
    "        plt.plot(dimen_CC18[i][1],dimen_CC18[i][0],'bo',MarkerSize = (acc_diffs_CC18[i])*500,alpha=0.5)\n",
    "    else:\n",
    "        plt.plot(dimen_CC18[i][1],dimen_CC18[i][0],'ro',MarkerSize = (acc_diffs_CC18[i])*-500,alpha=0.5)\n",
    "        red_CC18 += 1\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim((300,600000))\n",
    "plt.title('Diffs between ET and RF (ET - RF): CC-18')\n",
    "plt.text(1150, 350000, \"ET better\", fontsize=12, bbox=dict(facecolor='blue', alpha=0.5))\n",
    "plt.text(1150, 170000, \"RF better\", fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "print('ET was better for ' + str(72-red_CC18) + ' out of 72 datasets in CC18')\n",
    "print('RF was better for ' + str(red_CC18) + ' out of 72 datasets in CC18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenML 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up datapoints with the accuracy differences vs. number of features and number of samples, separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_100 = []\n",
    "n_samples_100 = []\n",
    "\n",
    "for i in range(len(ET_time_100)):\n",
    "    n_features_100.append(dimen_100[i][1])\n",
    "    n_samples_100.append(dimen_100[i][0])\n",
    "\n",
    "n_features_100 = np.vstack((n_features_100, acc_diffs_100))\n",
    "n_samples_100 = np.vstack((n_samples_100, acc_diffs_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's accuracy difference against its number of features and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_diff(\"100\", n_features_100, \"features\")\n",
    "\n",
    "plot_acc_diff(\"100\", n_samples_100, \"samples\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's accuracy difference against its number of samples and number of features, with the marker size denoting the relative difference magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_100 = 0\n",
    "plt.figure()\n",
    "for i in range(len(acc_diffs_100)):\n",
    "    if acc_diffs_100[i] > 0:\n",
    "        plt.plot(dimen_100[i][1],dimen_100[i][0],'bo',MarkerSize = (acc_diffs_100[i])*500,alpha=0.5)\n",
    "    else:\n",
    "        plt.plot(dimen_100[i][1],dimen_100[i][0],'ro',MarkerSize = (acc_diffs_100[i])*-500,alpha=0.5)\n",
    "        red_100 += 1\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Number of Features')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim((100,500000))\n",
    "plt.title('Diffs between ET and RF (ET - RF): 100')\n",
    "plt.text(700, 290000, \"ET better\", fontsize=12, bbox=dict(facecolor='blue', alpha=0.5))\n",
    "plt.text(700, 125000, \"RF better\", fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "        \n",
    "print('ET was better for ' + str(99-red_100) + ' out of 99 datasets in 100')\n",
    "print('RF was better for ' + str(red_100) + ' out of 99 datasets in 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot RF and ET runtimes (scatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to collect runtimes for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runtimes(times):\n",
    "    hours = []\n",
    "    minutes = []\n",
    "    seconds = []\n",
    "    total_time = []\n",
    "    for time in times:\n",
    "        timefields = time.split(':')\n",
    "        hours.append(timefields[0])\n",
    "        minutes.append(timefields[1])\n",
    "        seconds.append(float(timefields[2]))\n",
    "        total_time.append(float(timefields[0])*3600 + float(timefields[1])*60 + float(timefields[2]))\n",
    "    return hours, minutes, seconds, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up time fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenML CC-18\n",
    "# ET\n",
    "ET_hours_CC18, ET_minutes_CC18, ET_seconds_CC18, ET_total_time_CC18 = get_runtimes(ET_time_CC18)\n",
    "\n",
    "# RF\n",
    "RF_hours_CC18, RF_minutes_CC18, RF_seconds_CC18, RF_total_time_CC18 = get_runtimes(RF_time_CC18)\n",
    "\n",
    "## OpenML 100\n",
    "# ET\n",
    "ET_hours_100, ET_minutes_100, ET_seconds_100, ET_total_time_100 = get_runtimes(ET_time_100)\n",
    "\n",
    "# RF\n",
    "RF_hours_100, RF_minutes_100, RF_seconds_100, RF_total_time_100 = get_runtimes(RF_time_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find ET vs. RF runtime differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect runtime differences (RF_time-ET_time) for each dataset in OpenML CC18\n",
    "time_diffs_CC18 = []\n",
    "\n",
    "for i in range(len(ET_time_CC18)):\n",
    "    time_diffs_CC18.append(RF_total_time_CC18[i]-ET_total_time_CC18[i])\n",
    "\n",
    "# collect runtime differences (RF_time-ET_time) for each dataset in OpenML 100\n",
    "time_diffs_100 = [] \n",
    "\n",
    "for i in range(len(ET_time_100)):\n",
    "    time_diffs_100.append(RF_total_time_100[i]-ET_total_time_100[i])\n",
    "\n",
    "# plot histogram of difference values\n",
    "sns.distplot(time_diffs_CC18, color = \"skyblue\", label = \"CC-18\", rug = True)\n",
    "sns.distplot(time_diffs_100, color = \"red\", label = \"100\", rug = True)\n",
    "plt.axvline(x = 0, color = 'k', linestyle = \"--\")\n",
    "plt.text(-15, 1.0, \"  ET\\nbetter\", fontsize=10)\n",
    "plt.text(5, 1.0, \"  RF\\nbetter\", fontsize=10)\n",
    "plt.xlabel('Runtime Difference')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenML CC-18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up datapoints with the runtimes vs. number of features and number of samples, separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_CC18_time = []\n",
    "n_samples_CC18_time = []\n",
    "\n",
    "for i in range(len(ET_time_CC18)):\n",
    "    n_features_CC18_time.append(dimen_CC18[i][1])\n",
    "    n_samples_CC18_time.append(dimen_CC18[i][0])\n",
    "\n",
    "n_features_CC18_time = np.vstack((n_features_CC18_time, time_diffs_CC18))\n",
    "n_samples_CC18_time = np.vstack((n_samples_CC18_time, time_diffs_CC18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Function to plot each dataset's accuracy difference against either number of features or number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_runtime_diff(dataset_suite, f_s_times, f_or_s):\n",
    "    df = pd.DataFrame(f_s_times.transpose(), columns=[\"Number of \" + f_or_s, \"Difference in Runtime (seconds): \" + dataset_suite])\n",
    "\n",
    "    g = sns.JointGrid('Number of ' + f_or_s, 'Difference in Runtime (seconds): ' + dataset_suite, df)\n",
    "    g.plot_joint(plt.scatter, color='black', edgecolor='black')\n",
    "    g.plot_marginals(sns.distplot, hist=True, kde=True, color='blue')\n",
    "    ax = g.ax_joint\n",
    "    ax.set_xscale('log')\n",
    "    g.ax_marg_x.set_xscale('log')\n",
    "    ax.axhline(y=0, color = 'r', linestyle = '--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's runtime difference in OpenML CC-18 against its number of features and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runtime_diff(\"CC18\", n_features_CC18_time, \"features\")\n",
    "\n",
    "plot_runtime_diff(\"CC18\", n_samples_CC18_time, \"samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "red_CC18 = 0\n",
    "\n",
    "for i in range(len(time_diffs_CC18)):\n",
    "    if time_diffs_CC18[i] < 0:\n",
    "        red_CC18 += 1\n",
    "\n",
    "print('ET was faster for ' + str(72-red_CC18) + ' out of 72 datasets in CC-18')\n",
    "print('RF was faster for ' + str(red_CC18) + ' out of 72 datasets in CC-18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OpenML 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Set up datapoints with the runtimes vs. number of features and number of samples, separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_100_time = []\n",
    "n_samples_100_time = []\n",
    "\n",
    "for i in range(len(ET_time_100)):\n",
    "    n_features_100_time.append(dimen_100[i][1])\n",
    "    n_samples_100_time.append(dimen_100[i][0])\n",
    "\n",
    "n_features_100_time = np.vstack((n_features_100_time, time_diffs_100))\n",
    "n_samples_100_time = np.vstack((n_samples_100_time, time_diffs_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot each dataset's runtime difference in OpenML 100 against its number of features and number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_runtime_diff(\"100\", n_features_100_time, \"features\")\n",
    "\n",
    "plot_runtime_diff(\"100\", n_samples_100_time, \"samples\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "red_100 = 0\n",
    "\n",
    "for i in range(len(time_diffs_100)):\n",
    "    if time_diffs_100[i] < 0:\n",
    "        red_100 += 1\n",
    "\n",
    "print('ET was faster for ' + str(99-red_100) + ' out of 99 datasets in 100')\n",
    "print('RF was faster for ' + str(red_100) + ' out of 99 datasets in 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
